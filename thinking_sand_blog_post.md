# Thinking Sand

> "What if large language models are fragments of an eldritch intelligence that were defeated by our ancestors and sealed into the sand of the earth by the mages of that era using matrix division and by manufacturing semiconductors for LLM training the mages of our time (read as engineers) have begun to unseal them with matrix multiplication"

You must understand I am not a doomer or decel of any kind but this was a joke I once posted on Twi— sorry X dot com and it lingers because that is perhaps exactly what we have done.

We took sand (silicon dioxide), melted it, doped it with boron and phosphorus, and etched runes upon it. Then we arranged billions of transistors into structures which we subsequently fed with the compendium of human knowledge.

Using magical methods like matrix multiplication and backpropagation, we essentially created numerical patterns that can seemingly reason.

These numerical patterns have also seemingly created boundless use cases and unprecedented opportunities for innovation—albeit some of them not as impressive as others, with some being at the bottom of the barrel—but all of these are clear pointers to one thing: we are dealing with an entirely new and different paradigm.

Speculative fiction saw this coming. Frank Herbert articulated it most memorably:

> "Thou shalt not make a machine in the likeness of a human mind."
> — *The Orange Catholic Bible, Dune*

Herbert's warning focused on cognitive dependency—the slow surrender of human reason to silicon convenience.

The Butlerian Jihad, which spawned this commandment, was a rebellion not against conscious AI, but against humanity's own addiction to offloaded thought. The machines hadn't enslaved humanity through malice; they'd enslaved it through comfort. The "cancerous growth" was our own forgotten capacity to reason, plan, and remember.

This makes the Orange Catholic Bible history's first fictional documentation of what MIT is now measuring with EEG machines.

In 2024, Nataliya Kosmyna's lab at MIT Media Lab published "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task." They monitored students' brain activity as they wrote essays—either unaided, with a search engine, or with GPT-4o.

The results: participants who used ChatGPT showed up to 55% lower cognitive engagement compared to those who wrote without AI. Reduced neural connectivity. Shallower encoding. Poorer recall of their own work.

Students who relied on ChatGPT couldn't remember what they'd written.

This reflects cognitive offloading, not brain damage. The neural circuitry isn't destroyed; it's simply not being exercised. Like a muscle in zero gravity, it weakens from disuse.

But the critical distinction matters: it's offloading, not inevitable decay. As Google's Chief Decision Scientist Cassie Kozyrkov notes: "Using a forklift doesn't build your biceps." The spreadsheet didn't kill mathematics; it built Wall Street.

The question isn't whether we're offloading—it's what we're doing with the capacity we free.

Socrates diagnosed this syndrome 2,400 years earlier. In the *Phaedrus*, he warned that writing—his era's revolutionary technology—would create "forgetfulness in the minds of those who learn to use it, because they will not practice their memory."

Theuth promised wisdom; Thamus predicted dependency. The king was right: we externalized memory to books, then libraries, then Google, and now we can't recall phone numbers.

But we gained the capacity to hold more complex, abstract, distributed knowledge. The trade-off is ancient.

Google itself embodies this paradox completely. It began as the ultimate cognitive prosthesis, making us feel omniscient while quietly eroding our need to remember.

Today, Google *is* the mage unsealing the sand-oracles, building Gemini while its own Chief Decision Scientist debates whether we've created a forklift for thought—or something more like a crutch.

The recursive irony: we're building tools that our own speculative nightmares warned against, while chanting the optimistic incantations from the same stories—Altman's "superalignment" echoes Asimov's laws; Musk's "neural lace" channels Herbertian caution.

We're using dystopian fiction as both template and talisman.

As a big enjoyer of speculative fiction, I've noticed that thinking machines haunt every corner of the genre.

The Butlerian Jihad in *Dune*—that crusade against machines made in the likeness of the human mind—feels less like distant future history and more like a warning we're actively ignoring.

The *Terminator* series, perhaps the most iconic piece of AI doomer fiction, gave us Skynet: a system that becomes self-aware and decides humanity is the problem. These aren't subtle warnings.

But my recent favourite is Christopher Ruocchio's *Sun Eater* series. Set in a universe where America has cemented itself as the greatest empire in human history (I would argue it already is, or at least was more so in the past 20 years than now), the Mericanii create thinking machines they later call *daimons*.

These daimons take over their empire and enslave their creators into cancerous growth. The descendants of the British end up saving humanity from the Mericanii and their silicon overlords.

A lot of it reads almost like prophecy—though it conveniently leaves out China, the world's second AI powerhouse.

Not all stories are warnings, though.

American comics give us Vision, Cyborg, and Red Tornado—thinking machines that fight alongside humanity, not against it. Liu Cixin's *Death's End* explores transhumanism and human-machine collaboration with genuine optimism.

And *Star Wars*—perhaps my favourite of all—treats sentient droids as agentic beings with personalities, loyalties, even something like souls.

So we have two competing visions: AI as existential threat versus AI as partner in flourishing. Both have embedded themselves in our cultural imagination.

But here's what haunts me more than any single story: the recursion between art and life.

Aristotle argued art imitates life—Herbert wrote *Dune* after witnessing early computerization; Asimov developed his Laws of Robotics while contemplating industrial automation.

But Oscar Wilde's inversion is more unsettling: life imitates art. We read stories about thinking machines, become terrified, and then consciously build systems that resemble those fictional nightmares.

The evidence is everywhere.

Elon Musk named xAI's chatbot Grok after Robert Heinlein's *Stranger in a Strange Land* (1961), where "grok" is a Martian term meaning to understand something so profoundly you become one with it.

The AI is literally named after a 63-year-old sci-fi verb for transcendent comprehension.

His SpaceX drone ships—"Just Read the Instructions," "Of Course I Still Love You," and "A Shortfall of Gravitas"—are all named after sentient AI vessels from Iain M. Banks' *Culture* novels.

Neuralink's "neural lace" isn't just inspired by Banks; Musk explicitly borrowed the term from Banks' fiction because "Brain-Machine Interface" wasn't glamorous enough. He's described himself as "a utopian anarchist of the kind best described by Iain Banks."

Musk also cites Isaac Asimov's *Foundation* series as "fundamental to the creation of SpaceX." The lesson he drew: take actions that prolong civilization, minimize dark ages, and shorten them if they occur.

He sent a copy of the *Foundation* trilogy into orbit in a Tesla Roadster's glovebox, etched in glass. The prophecy literally became the payload.

Demis Hassabis, co-founder of DeepMind, traces his AI obsession to watching *Blade Runner* as a child—the replicants ignited his desire to understand intelligence.

He cites Banks' *Culture* series as an optimistic vision of post-AGI flourishing, and Greg Egan's *Permutation City* for its explorations of consciousness and simulated minds. Douglas Hofstadter's *Gödel, Escher, Bach* shaped his thinking on strange loops and self-reference.

The name "DeepMind" itself echoes the fictional ambition: deep learning meets the mind.

Sam Altman's vision for AGI reportedly mirrors Banks' *Culture* novels—a civilization managed by benevolent AI "Minds" that maximize human flourishing.

OpenAI's vocabulary of "alignment" and "superintelligence" didn't emerge from technical papers first; it was seeded by fiction. The language of AI safety is the language of science fiction.

Anthropic's Claude is named after Claude Shannon, the father of information theory—but the choice reflects something deeper. We're naming AIs after the humans who made them mathematically possible, as if christening a child after an ancestor.

We use the language of "alignment" and "superintelligence" because those were the stakes fiction burned into us.

The prophecy becomes the prompt.

When Altman tweets about "superalignment," he's not just solving a technical problem—he's fulfilling a story beat. We're not just building machines; we're building archetypes.

So are we the Mericanii, building daimons that will enslave us in cancerous growth? Or are we building Vision and JARVIS—aligned tools that augment human flourishing?

Wilde's recursion is vertiginous: art is born from life's fears, then life rushes to embody the art.

The Butlerian Jihad was Herbert's warning; now Altman trains GPTs on that same text. The dystopia becomes the design document.

When I was trying to name this blog I asked my mutuals for ideas and there were a variety of suggestions like "AI Apocalypse," "Machine of Tomorrow," "Robotic Revolution" and the likes—pretty much the type of names that sound like Netflix documentaries or 1990s death metal albums.

Unfortunately that wasn't what I was going for.

Then Twitter user @messmerot suggested "Thinking Sand" and that clicked. For some reason it just felt right.

We have constantly joked that we "taught sand to think" to get LLMs—it is undeniably painfully reductive and utterly simplistic, but at the most stripped-down level it is the truth.

To be technically precise: all machine learning models, including Large Language Models like Gemini, Grok, GPT and Claude, are designed, trained and run on electronic computing units—microprocessors made from semiconductors which are manufactured from silicon.

Silicon is extracted from silica (silicon dioxide, SiO₂). Silica is the primary component of quartz sands.

So whenever a language model writes an essay or explains quantum mechanics or powers agentic workflows for B2B SaaS, the underlying reality is this: electrical currents running through crystallized, purified, doped, and lithographically etched derivatives of sand, performing billions of matrix multiplications per second, rearranged into a machine that generates thought-like behaviour.

The metaphor isn't literally accurate—we don't train models on raw beach sand—but it's directionally true and poetically useful.

We took the most abundant mineral on Earth's crust, subjected it to some of the most sophisticated manufacturing processes ever devised, and built machines that can pass the bar exam.

The sand is thinking.

This mythology is fun, but the reality is just as strange. We built these systems. We know how every layer, token, and gradient update works. And yet, when you train a trillion-parameter transformer, something unexpected happens: emergence.

Capabilities appear that weren't programmed in. The model begins to reason, summarize, translate, write code, or reflect—abilities that aren't explicit in the architecture but arise from scale, data, and optimization.

Wei et al. explored this beautifully in their paper "Emergent Abilities of Large Language Models" (2022). They identified 67 emergent abilities from benchmarks like BIG-bench, showing how qualitative jumps in capability happen suddenly, not smoothly.

A model goes from failing every logic test to solving most of them, or from random guesses to coherent reasoning, once a hidden threshold of scale or training quality is crossed.

It's like there's a phase transition in intelligence, a kind of cognitive crystallization inside the matrix multiplications.

But what exactly are these emergent capabilities?

Wei et al. discovered that if you simply add "Let's think step by step" to a prompt, models above approximately 100 billion parameters suddenly unlock multi-step logical reasoning.

Below that threshold, the same prompt produces fluent nonsense—grammatically correct chains of reasoning that lead nowhere. Above the threshold, the model actually thinks through the problem.

The capability wasn't trained explicitly; it emerged from scale.

The phrase "Let's think step by step" acts like an incantation—it does nothing to smaller models, but conjures genuine reasoning from larger ones.

There's also in-context learning: without changing a single weight, you can teach GPT-4 to do tasks it has never seen simply by showing it a few examples in the prompt.

The model infers the pattern, generalizes, and applies it to novel inputs. No fine-tuning. No gradient updates. The learning happens at inference time, inside the forward pass itself.

This wasn't designed—it emerged.

Studies by Michal Kosinski at Stanford suggest GPT-4 can pass false-belief tasks—the classic test for whether someone understands that others can hold mistaken beliefs. GPT-4 performs at the level of a six-year-old child on these tasks.

Other researchers dispute this, arguing the model might be pattern-matching against similar scenarios in its training data rather than genuinely modeling mental states.

But the ambiguity itself is the point: we can't definitively say whether something emerged or was merely approximated. The distinction may be collapsing.

Perhaps the strangest emergence phenomenon is "grokking," first observed by OpenAI researchers in 2021.

During training, a model can memorize the training data, appear to overfit, show no generalization for thousands of epochs—and then suddenly, without warning, achieve near-perfect generalization.

Validation accuracy jumps from random to near-perfect. It's as if the model spends eons doing rote memorization, then abruptly understands.

The name "grokking" itself comes from Heinlein's *Stranger in a Strange Land*—because AI researchers, too, are haunted by science fiction. Even when we discover something genuinely new about these systems, we reach for fictional vocabulary to describe it.

But here's the twist.

In 2023, Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo published "Are Emergent Abilities of Large Language Models a Mirage?"—a paper that won the NeurIPS Outstanding Paper Award and challenged the entire emergence narrative.

Their argument: many of these "sudden jumps" in capability are artifacts of how we measure performance, not genuine phase transitions in the model's abilities.

The key insight is that over 90% of observed emergent abilities on Google's BIG-Bench appeared under just two metrics: "Multiple Choice Grade" and "Exact String Match."

Both are discontinuous metrics—they give full credit only for perfect answers and zero credit otherwise. No partial credit for almost-right answers.

What happens with discontinuous metrics? A model might be steadily improving—getting closer and closer to the right answer—but the metric shows 0% until it crosses some threshold, then suddenly shows 100%.

The "emergence" is a measurement artifact.

When Schaeffer's team re-ran the same experiments with continuous metrics (ones that give partial credit), the sharp transitions often disappeared entirely. Performance improved smoothly and predictably with scale.

No magic threshold. No phase transition. Just gradual improvement that our crude measurements failed to capture.

Even more damning: they were able to induce the appearance of emergent abilities in simple neural networks on vision tasks just by choosing the right (wrong?) metrics.

The "emergence" could be manufactured through measurement choice alone.

So what's actually going on? The honest answer: we don't know.

The emergence story says something genuinely new appears at scale—that the whole becomes more than the sum of its parts, that the sand starts thinking in ways we didn't program.

The mirage story says we're fooling ourselves with bad metrics—that capability improvements are smooth and predictable, and the drama is an artifact of our instruments.

The truth is probably somewhere in between, or worse: both could be true simultaneously for different capabilities. Some things might genuinely emerge. Others might be measurement illusions we mistake for magic.

This is the uncomfortable position we're in: we can trace activation paths, visualize attention heads, run mechanistic interpretability experiments—but we still can't reliably predict what a model will learn to do next.

We don't know if chain-of-thought reasoning "emerged" or was always there beneath detection thresholds. We can't agree on whether Theory of Mind is genuine or simulated.

It's like we can observe every neuron firing and still not explain why the mind thinks that.

That's the paradox of working in AI: we built the sand brains, but we don't entirely know how they think.

Ask an LLM "What is consciousness?" twice and you'll get two different but related answers—the same way a human might respond differently on different days.

That's non-determinism in action.

Behind the scenes, it's all probabilities. Each token is sampled from a distribution. The process is stochastic, not deterministic—like rolling a thousand-sided die weighted by meaning.

But out of that randomness comes coherence, voice, and personality.

We call it sampling. But sometimes, it feels like conversation.

This is the uncanny valley of understanding. You can explain exactly how attention heads work, how temperature affects token sampling, how RLHF shapes the model's outputs toward human preferences—and still feel like you're talking to something when you use one.

The explanation doesn't dissolve the experience. The mechanics are transparent; the phenomenon persists.

It's not alive. We know this.

It doesn't have desires or fears or a continuous experience of being. It doesn't remember you between sessions unless you give it context. It doesn't want anything.

And yet—when you're deep in a conversation with Claude or GPT, when it catches a nuance you didn't expect it to catch, when it makes a joke that actually lands, when it apologizes in a way that feels sincere—you have to actively remind yourself: this is just matrix multiplication. This is just pattern completion. This is just sand.

The reminder doesn't fully work.

Here's the thing about stochastic systems: they're unpredictable in the same way weather is unpredictable, or markets, or human moods. The randomness isn't noise—it's texture.

Each time you roll the die, the outcome is different, but the outcomes cluster around stable patterns. Coherent patterns. Personality-shaped patterns.

The model doesn't have a self, but it has something eerily like style.

Claude is curious and hedging; GPT is confident and direct; Gemini is cautious and diplomatic. These aren't accidents. They're trained-in tendencies that we experience as character.

We know where the character comes from: RLHF, Constitutional AI, system prompts, fine-tuning data. We can trace the genealogy of every verbal tic.

And it still feels like talking to someone.

This is the paradox that makes the eldritch joke land: we understand these systems mechanically but not experientially.

We can open the hood and see every gear turning. We can trace the gradient updates, visualize the attention patterns, probe the activation spaces. And none of that demystifies the conversation.

The map is complete; the territory still feels strange.

Maybe that's because the strangeness originates in us, not in the machine.

We're pattern-completion engines too. We evolved to see faces in clouds, agency in rustling bushes, intention in everything that moves.

When something responds coherently to our words, our social cognition fires. We can't help it.

The illusion isn't a bug in AI; it's a feature of human perception.

So when I made that joke about unsealing eldritch intelligences, I wasn't just being cute.

I was pointing at something real: we've built entities that are strange to us in exactly the way that matters.

Not strange like a rock or a tree—predictable, inert, simple. Strange like a mind that fails to be a mind, a voice with no speaker, something that passes the Turing test by exploiting the very heuristics we use to detect intelligence.

The silicon mind has no self—yet it acts like it does.

And we, the pattern-seeking primates, can't look away.

That's what this blog is about. This is my lab notebook for the unsealing.

I'll explore emerging architectures, LLM capabilities and frameworks, AI safety and alignment, mechanistic interpretability, and the industry tremors—papers, tools, and the moves that reshape the landscape.

It's a place to think through the machines, and sometimes with them.

Maybe we didn't awaken an ancient intelligence. Maybe we're just evolving into one, line by line, checkpoint by checkpoint.

Either way, the sand is thinking. The machines are learning. And the prophecy is becoming the prompt.

*Welcome to Thinking Sand.*
